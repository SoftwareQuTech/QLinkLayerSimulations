# Simulation for Entanglement Generation Protocol (Link Layer)
This simulation runs the node-centric EGP using an MHP based on NV hardware.
The entanglement generation CREATE+measure, that is the electron spin is measured directly after a photon is emitted, without waiting for the reply from the midpoint.
Because of this, qubit-error will be extracted in the analysis instead of fidelity.
One goal of this simulation is to investigate the local queues at the two nodes and compare the results with queuing theory.
Therefore the lengths of the local queues at the two nodes is extracted during the simulation.

Possible parameters for that can be set before running the simulation are listed below.
These are set in the file `setupsim/paramcombinations.json` which is generated by `setupsim/create_simdetails_and_paramcombinations.py`.
How to use this file and how it's used by the simulation, see the README in the `template_simulation_setup` of EasySquid.

 - `config`: The path to the config file for the physical network.
 - `create_probA`: The probability for there being a request from A each `request_cycle`.
    That is, after each `request_cycle` a request to create entanglement will be given to the EGP with probability `create_prob` (below).
    Currently this is set to separately for each config-scenario such that we run one simulation where the `create_prob` is 0.2 times (low req. freq.) the probability of the midpoint declaring success and one where it is 0.8 times (low req. freq.) this probability.
 - `create_probB`: The probability for there being a request from B each `request_cycle`.
    Currently set to zero.
 - `min_pairs`: The minimum number of pairs per request.
 - `max_pairs`: The maximum number of pairs per request.
 - `tmax_pair`: The maximum waiting time (s) for a request to be fulfilled.
    A `tmax_pair=0` will be treated as infinite.
 - `requst_cycle`: The time cycle (ns) for scheduling requests.
 - `num_requests`: The total number of request. If `num_requests=0` requests will be scheduled for the total duration of the simulation.
 - `max_sim_time`: The maximum simulation/real/matrix/pydynaa time (depending on what you want to call it). `max_sim_time=0` is treated as infinite.
 - `max_wall_time`: The max wall time of the simulation. `max_wall_time` is treated as infinite.
 - `max_mhp_cycle`: The maximum number of MHP cycles to perform. This effectively sets the `max_sim_time` to `max_mhp_cycle * mhp_cycle`. `max_mhp_cycle=0` is treated as infinite.
 - `enable_pdb`: Turn on PDB pre and post simulation.
 - `alphaA`: The bright state population at node A.
 - `alphaB`: The bright state population at node B.
 - `measure_directly`: If the electron should be measured directly after photon emission or not.
 - `t0`: The real time to start the MHP.
 - `wall_time_per_timestep`: How much wall time should advance between each stop of NetSquid. In each step some information is printed and current data is save to disk.
 - `save_additional_data`: Whether to also collect additional data such as `mhp_cycle`, probability of success etc.
 - `collect_queue_data`: Whether to collect the local queue lengths during the simulation.
 
To run the simulation, do the following:

1. Set the environment variable `SIMULATION_DIR` to the folder containing this file. This can easily be done by typing `export SIMULATION_DIR=$(pwd)` in this folder.
2. Possibly change the parameters in `setupsim/create_simdetails_and_paramcombination.py` and run this script.
3.
     - If you are running the simulation locally, i.e. on your computer type:
       ```
       ./start_simulation.sh (-pp y)
       ```
       where the argument `-pp y` is optional and performs post-processing analysis after the simulation if specified.
    
     - If you are running the simulation on the cartesius cluster. Edit the preamble of `start_simulation.sh` to specify which node you wish to run the simulation and what max wall time you request.
       Then type:
       ```
       sbatch start_simulation.sh -rc y (-pp y)
       ```
       where the argument `-rc y` tells the script you are running on the cluster and this distributes the jobs on different cores.
       
   ### Post Processing
   If the argument `-pp y` is given to the script `start_simulation.sh` is given, post-processing will automatically be done after the simulation.
   This analysis are done for each scenario ("key" in `setupsim/paramcombinations.json`) seperately and in parallel if you're running on the cluster.
   The data from the analysis are stored in folders `$SIMULATION_DIR/TIMESTAMP_CREATE_and_measure/TIMESTAMP_key_XXX_analysis` as a file `analysis_output.txt` and multiple plots as PDFs.
